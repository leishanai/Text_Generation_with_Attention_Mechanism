{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprossessing\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import numpy as np\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "# helper\n",
    "import time\n",
    "import math\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "# cost_function plot\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class voc:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'PAD', 1: 'SOS', 2: 'EOS'}\n",
    "        self.n_words = 3  # Count PAD, SOS and EOS\n",
    "        self.max_length = 1\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def addSentence(self, sentence):\n",
    "        i = 0\n",
    "        for word in sentence.split():\n",
    "            self.addWord(word)\n",
    "            i+=1\n",
    "        if i > self.max_length:\n",
    "            self.max_length = i    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Clean text: remove punctuations, truncate the length and group into pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):        \n",
    "    '''\n",
    "    Return cleaned a list of pairs, e.g. [[pair1, pair2], [pair3, pair4], ...]\n",
    "    '''\n",
    "    # Read the file and split into lines\n",
    "    lines = open(data_path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = []\n",
    "    for i in range(0,len(lines),2):\n",
    "        pairs.append([normalizeString(lines[i]), normalizeString(lines[i+1])])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the pairs by max_length\n",
    "def filterPair(p, MAX_LENGTH):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs, MAX_LENGTH):\n",
    "    return [pair for pair in pairs if filterPair(pair, MAX_LENGTH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data_path, MAX_LENGTH):\n",
    "    '''\n",
    "    Return voc instance, and trimed and cleaned pairs\n",
    "    '''\n",
    "    pairs = read_data(data_path)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs, MAX_LENGTH)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    # initialize voc instance\n",
    "    lang = voc()\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        lang.addSentence(pair[0])\n",
    "        lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(lang.n_words)\n",
    "    return lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 3406 sentence pairs\n",
      "Trimmed to 3127 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "5532\n",
      "['this boulder on my shoulder gets heavy and harda to hold', 'and this load is like the weight of the world']\n"
     ]
    }
   ],
   "source": [
    "# test things out\n",
    "data_path = '../data/eminem.txt'\n",
    "lang, pairs = prep_data(data_path, MAX_LENGTH = 20)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Tensorization (pair -> split -> add EOS token -> tensor -> integer encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to index\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split()]\n",
    "# add EOS_token at the end of sentence\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    # convert indexes into torch.long datatype\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "# split pair into input and target\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to import glove\n",
    "# from torchnlp.word_to_vector import GloVe\n",
    "\n",
    "def pretrained(glove_path):\n",
    "    '''\n",
    "    load pretrained glove and return it as a dictionary\n",
    "    '''\n",
    "    # dimension of import word2vec file\n",
    "    glove = {}\n",
    "    with open(glove_path,'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove[word] = coefs\n",
    "    return glove\n",
    "\n",
    "def embeds(glove_path, EMBEDDING_DIM, non_trainable=True):\n",
    "    '''\n",
    "    match pretrained embedding and dataset\n",
    "    '''\n",
    "    glove = pretrained(glove_path)\n",
    "    embedding_matrix = np.zeros((lang.n_words, EMBEDDING_DIM)) # initialization\n",
    "    # count words that appear only in the dataset. word_index.items() yields dict of word:index pair\n",
    "    for word, ix in lang.word2index.items():\n",
    "        embedding_vector = glove.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in glove matrix will be all-zeros.\n",
    "            embedding_matrix[ix] = embedding_vector\n",
    "    # change the datatype from numpy_array to torch_tensor\n",
    "    weight = torch.FloatTensor(embedding_matrix)\n",
    "    # convert to the embedding of pytorch\n",
    "    embedding_matrix = nn.Embedding.from_pretrained(weight)\n",
    "    # make it non-trainable\n",
    "    if non_trainable:\n",
    "        embedding_matrix.weight.requires_grad = False\n",
    "    \n",
    "    return embedding_matrix, glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic decoder\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # self.embedding = nn.Embedding(input_size, hidden_size) # from scratch\n",
    "        self.embedding, _ = embeds(glove_path, EMBEDDING_DIM, True) # load pretrained embedding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    # this forward function only takes one sample\n",
    "    def forward(self, input, hidden):\n",
    "        # for one word, embedded vector in 3D tensor is reshaped into (1, 1, -1)\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention implemented decoder\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "#         nn.Module.__init__(self)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # self.embedding = nn.Embedding(self.output_size, self.hidden_size) # from sratch\n",
    "        self.embedding, _ = embeds(glove_path, EMBEDDING_DIM, True) # load pretrained embedding\n",
    "        # attention layer\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # gru layer\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # embedded layer\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # attention layer\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) # concat hidden unit and embedded vector\n",
    "        # apply weights on encoder outputs\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        # concat embedded vector and weighted decoder output\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # relu + gru\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # softmax out\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "# this function only trains on one pair\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    # reset memory state at the start of taking a new input\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    # cleanup encoder/decoder grads\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    # need to save encoder_outputs, so we initialized it as zero matrix first\n",
    "    # encoder_outputs will be using to calculate attention weights\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        # note that encoder_output is a vector with length = hidden_size\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        # append output to encoder_outputs\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    #################################################################################\n",
    "    # decoder part\n",
    "    # decoder_input starts with SOS_token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    # first decoder_hidden = last encoder_hidden\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            # feed input into decoder which yields output, next hidden unit, decoder attention\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # obtain loss from optimizer\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            # collect info for obtaining accuracy\n",
    "            total +=1\n",
    "            # greedy search\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi == target_tensor[di]: correct +=1\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # collect info for obtaining accuracy\n",
    "            total +=1\n",
    "            # greedy search\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi == target_tensor[di]: correct +=1\n",
    "            # break if EOS_token generated\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "    # accumulate gradient by virtue of backpropagation\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length, correct, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    '''\n",
    "    This function will run on one sample\n",
    "    input_tensor, target_tensor -> integer encoded\n",
    "    encoder, decoder -> instantiated encoder and decoder\n",
    "    criterion -> will be given in trainIter\n",
    "    max_length -> required to build encoder_outputs\n",
    "    \n",
    "    returns: averaged loss, correctly predicted words, total words predicted\n",
    "    '''\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # prepare input from encoder end\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        # initiate output matrix from encoder\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            # append output to encoder_outputs matrix\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "            \n",
    "        ########################################################################\n",
    "        # decoder part\n",
    "        # first input of decoder is SOS_token\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        # hidden unit is the last hidden unit from encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # for loop for each word in the sequence\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # a.topk(n) yields top n items from a and corresponding index of items\n",
    "            # greedy search output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            if topi == target_tensor[di]: correct +=1\n",
    "            # accumulate loss\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # .item() convert datatype from torch tensor to int\n",
    "            if topi.item() == EOS_token: break\n",
    "            \n",
    "        return loss.item()/target_length, correct, target_length\n",
    "\n",
    "def validIter(testing_pairs, encoder, decoder, criterion):\n",
    "    '''\n",
    "    testing_pairs -> integer encoded input\n",
    "    encoder/decoder -> instantiated encoder/decoder\n",
    "    criterion -> defined in trainIter\n",
    "    \n",
    "    returns: averaged loss, averaged accuracy\n",
    "    '''\n",
    "    correct_total = 0\n",
    "    total_total = 0\n",
    "    loss_total = 0\n",
    "    data_size = len(testing_pairs)\n",
    "    \n",
    "    for itr in range(data_size):\n",
    "\n",
    "        pair = testing_pairs[itr]\n",
    "        input_tensor = pair[0]\n",
    "        target_tensor = pair[1]\n",
    "        # obtain loss from train function\n",
    "        loss, correct, total = validation(input_tensor, target_tensor, encoder,\n",
    "                     decoder, criterion, max_length=MAX_LENGTH)\n",
    "        # accumulate metrics\n",
    "        correct_total += correct\n",
    "        total_total += total\n",
    "        loss_total += loss\n",
    "    \n",
    "    return loss_total/data_size, correct_total/total_total * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Iteration of train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, epochs=2, learning_rate=0.01):\n",
    "    \n",
    "    # store loss of train and test\n",
    "    loss_array_train, loss_array_test = [], []\n",
    "    \n",
    "    # create optimizer for encoder and decoder\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # train/test split\n",
    "    pairs_train, pairs_test = train_test_split(pairs, test_size=0.2, random_state=666)\n",
    "    # randomly select pairs from training set and have them integer encoded\n",
    "    # epoch is meaningless in this case\n",
    "    # training_pairs = [tensorsFromPair(random.choice(pairs_train)) for i in range(n_iters)]    \n",
    "    training_pairs = [tensorsFromPair(pair) for pair in pairs_train]\n",
    "    testing_pairs = [tensorsFromPair(pair) for pair in pairs_test]\n",
    "\n",
    "    # negative log-likelihood\n",
    "    # criterion = nn.NLLLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    batch_size = 20\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Training on epoch_{}'.format(epoch+1))\n",
    "        \n",
    "#         # loss and acc of training set\n",
    "#         loss_train, acc_train = batchIter(training_pairs, encoder, decoder, encoder_optimizer, \n",
    "#         decoder_optimizer, criterion, batch_size, max_length=MAX_LENGTH)\n",
    "        \n",
    "        # loss and acc of testing set \n",
    "        loss_test, acc_test = validIter(testing_pairs, encoder, decoder, criterion)            \n",
    "        \n",
    "        # append losses to arrays\n",
    "        loss_array_train.append(loss_test)\n",
    "        loss_array_test.append(loss_test)\n",
    "        # verbose\n",
    "        print('<= Done! => loss_train:{}, acc_train:{}%, loss_test:{}, acc_test:{}%'\n",
    "              .format(round(loss_test,2), round(acc_test, 2), round(loss_test,2), round(acc_test, 2)))\n",
    "\n",
    "    showPlot(loss_array_train)\n",
    "    showPlot(loss_array_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 3406 sentence pairs\n",
      "Trimmed to 3127 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "5532\n"
     ]
    }
   ],
   "source": [
    "glove_path = 'glove.6B.50d.txt'\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "hidden_size = EMBEDDING_DIM\n",
    "\n",
    "lang, pairs = prep_data('eminem.txt', MAX_LENGTH = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_test = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
    "decoder_test = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on epoch_1\n",
      "<= Done! => loss_train:8.66, acc_train:0.0%, loss_test:8.66, acc_test:0.0%\n",
      "Training on epoch_2\n",
      "<= Done! => loss_train:8.66, acc_train:0.0%, loss_test:8.66, acc_test:0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder_test, decoder_test, 2, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # prepare input from encoder end\n",
    "        input_tensor = tensorFromSentence(lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        # initiate output matrix from encoder\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            # append output to encoder_outputs matrix\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        # first input of decoder is SOS_token\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        # hidden unit is the last hidden unit from encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # initialize output list from decoder\n",
    "        decoded_words = []\n",
    "        # initialize attention matrix\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # append attention\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            # a.topk(n) yields top n items from a and corresponding index of items\n",
    "            # greedy search output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # .item() convert datatype from torch tensor to int\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                # index to word\n",
    "                decoded_words.append(lang.index2word[topi.item()])\n",
    "            # new input is the output from last word\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f11a76a1a20>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(encoder1, attn_decoder1, \"i am rap god\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = i am rap god\n",
      "output = i <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"i am rap god\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "    batch_size = input_batches.size(1)\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    # max_length of batch data\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss for batch data\n",
    "    loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    target_batches.transpose(0, 1).contiguous(),\n",
    "    target_lengths)\n",
    "    \n",
    "    # accumulate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms to prevent exploding gradient issue\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlpy36]",
   "language": "python",
   "name": "conda-env-dlpy36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
